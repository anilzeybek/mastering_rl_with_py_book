{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from food_truck_env import FoodTruck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, policy):\n",
    "    prob_a = policy[state]\n",
    "    action = np.random.choice(a=list(prob_a.keys()), p=list(prob_a.values()))\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_visit_return(returns, trajectory, gamma):\n",
    "    \"\"\"\n",
    "    Returns is a dict that has states as keys and values of these states as values\n",
    "    Trajectory is a sample trajectory including state, action, reward in each element\n",
    "\n",
    "    Starts from last element of trajectory and calculates G for each state. Then appends this G to returns for corresponding state returns. \n",
    "    \"\"\"\n",
    "    G = 0\n",
    "    T = len(trajectory) - 1\n",
    "    for t, sar in enumerate(reversed(trajectory)):\n",
    "        s, a, r = sar\n",
    "        G = r + gamma * G\n",
    "\n",
    "        first_visit = True\n",
    "        for j in range(T - t):\n",
    "            if s == trajectory[j][0]:\n",
    "                first_visit = False\n",
    "\n",
    "        if first_visit:\n",
    "            if s in returns:\n",
    "                returns[s].append(G)\n",
    "            else:\n",
    "                returns[s] = [G]\n",
    "\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trajectory(env, policy):\n",
    "    \"\"\"\n",
    "    Creates a trajectory in given environment and policy\n",
    "    \"\"\"\n",
    "    trajectory = []\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    sar = [state]\n",
    "    \n",
    "    while not done:\n",
    "        action = choose_action(state, policy)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        sar.append(action)\n",
    "        sar.append(reward)\n",
    "        trajectory.append(sar)\n",
    "        \n",
    "        sar = [state]\n",
    "    return trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_visit_mc(env, policy, gamma, n_trajectories):\n",
    "    \"\"\"\n",
    "    Finds the state-value function for given policy\n",
    "    \"\"\"\n",
    "    returns = {}\n",
    "    v = {}\n",
    "    for i in range(n_trajectories):\n",
    "        trajectory = get_trajectory(env, policy)\n",
    "        returns = first_visit_return(returns, trajectory, gamma)\n",
    "        \n",
    "    for s in env.state_space:\n",
    "        if s in returns:\n",
    "            v[s] = np.round(np.mean(returns[s]), 1)\n",
    "            \n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy defined below is same we used in dynamic programming notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def some_policy(states):\n",
    "    policy = {}\n",
    "    for s in states:\n",
    "        day, inventory = s\n",
    "        prob_a = {}\n",
    "        \n",
    "        if inventory >= 300:\n",
    "            prob_a[0] = 1\n",
    "        else:\n",
    "            prob_a[200 - inventory] = 0.5\n",
    "            prob_a[300 - inventory] = 0.5\n",
    "\n",
    "        policy[s] = prob_a\n",
    "    \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = FoodTruck()\n",
    "policy = some_policy(env.state_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected weekly profit for some policy is:  2478.1\n"
     ]
    }
   ],
   "source": [
    "v_estimate = first_visit_mc(env, policy, 1, 1000)\n",
    "print(\"Expected weekly profit for some policy is: \", v_estimate[\"Mon\", 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again see that MC prediction predicted weekly profit given policy correct "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eps_greedy(actions, eps, a_best):\n",
    "    \"\"\"\n",
    "    Assigns probability to each action\n",
    "    \n",
    "    If there are 4 actions and eps=0.4, Best action gets 0.7 probability and other actions get 0.1 probability\n",
    "    \"\"\"\n",
    "    prob_a = {}\n",
    "    for a in actions:\n",
    "        if a == a_best:\n",
    "            prob_a[a] = 1 - eps + eps / len(actions)\n",
    "        else:\n",
    "            prob_a[a] = eps / len(actions)\n",
    "            \n",
    "    return prob_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_policy(states, actions):\n",
    "    policy = {}\n",
    "    for s in states:\n",
    "        policy[s] = {a: 1 / len(actions) for a in actions}\n",
    "        \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_policy_first_visit_mc(env, n_iter, eps, gamma):\n",
    "    states =  env.state_space\n",
    "    actions = env.action_space\n",
    "    policy =  get_random_policy(states, actions)\n",
    "    \n",
    "    Q = {s: {a: 0 for a in actions} for s in states}\n",
    "    N = {s: {a: 0 for a in actions} for s in states}\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "        if i % 10000 == 0:\n",
    "            print(f\"Iteration: {i}\")\n",
    "        \n",
    "        trajectory = get_trajectory(env, policy)\n",
    "        \n",
    "        G = 0\n",
    "        T = len(trajectory) - 1\n",
    "        for t, sar in enumerate(reversed(trajectory)):\n",
    "            s, a, r = sar\n",
    "            G = r + gamma * G\n",
    "            first_visit = True\n",
    "            \n",
    "            for j in range(T - t):\n",
    "                s_j = trajectory[j][0]\n",
    "                a_j = trajectory[j][1]\n",
    "                if (s, a) == (s_j, a_j):\n",
    "                    first_visit = False\n",
    "                    \n",
    "            if first_visit:\n",
    "                Q[s][a] = N[s][a] * Q[s][a] + G\n",
    "                N[s][a] += 1\n",
    "                Q[s][a] /= N[s][a]\n",
    "                \n",
    "                a_best = max(Q[s], key=Q[s].get)\n",
    "                policy[s] = get_eps_greedy(actions, eps, a_best)\n",
    "                \n",
    "    return policy, Q, N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Iteration: 10000\n",
      "Iteration: 20000\n",
      "Iteration: 30000\n",
      "Iteration: 40000\n",
      "Iteration: 50000\n",
      "Iteration: 60000\n",
      "Iteration: 70000\n",
      "Iteration: 80000\n",
      "Iteration: 90000\n",
      "Iteration: 100000\n",
      "Iteration: 110000\n",
      "Iteration: 120000\n",
      "Iteration: 130000\n",
      "Iteration: 140000\n",
      "Iteration: 150000\n",
      "Iteration: 160000\n",
      "Iteration: 170000\n",
      "Iteration: 180000\n",
      "Iteration: 190000\n",
      "Iteration: 200000\n",
      "Iteration: 210000\n",
      "Iteration: 220000\n",
      "Iteration: 230000\n",
      "Iteration: 240000\n",
      "Iteration: 250000\n",
      "Iteration: 260000\n",
      "Iteration: 270000\n",
      "Iteration: 280000\n",
      "Iteration: 290000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{('Mon', 0): {0: 0.01, 100: 0.01, 200: 0.01, 300: 0.01, 400: 0.96},\n",
       " ('Tue', 0): {0: 0.01, 100: 0.01, 200: 0.01, 300: 0.01, 400: 0.96},\n",
       " ('Tue', 100): {0: 0.01, 100: 0.01, 200: 0.96, 300: 0.01, 400: 0.01},\n",
       " ('Tue', 200): {0: 0.01, 100: 0.01, 200: 0.96, 300: 0.01, 400: 0.01},\n",
       " ('Tue', 300): {0: 0.01, 100: 0.96, 200: 0.01, 300: 0.01, 400: 0.01},\n",
       " ('Wed', 0): {0: 0.01, 100: 0.01, 200: 0.01, 300: 0.01, 400: 0.96},\n",
       " ('Wed', 100): {0: 0.01, 100: 0.01, 200: 0.96, 300: 0.01, 400: 0.01},\n",
       " ('Wed', 200): {0: 0.01, 100: 0.01, 200: 0.96, 300: 0.01, 400: 0.01},\n",
       " ('Wed', 300): {0: 0.01, 100: 0.96, 200: 0.01, 300: 0.01, 400: 0.01},\n",
       " ('Thu', 0): {0: 0.01, 100: 0.01, 200: 0.01, 300: 0.96, 400: 0.01},\n",
       " ('Thu', 100): {0: 0.01, 100: 0.01, 200: 0.96, 300: 0.01, 400: 0.01},\n",
       " ('Thu', 200): {0: 0.01, 100: 0.96, 200: 0.01, 300: 0.01, 400: 0.01},\n",
       " ('Thu', 300): {0: 0.96, 100: 0.01, 200: 0.01, 300: 0.01, 400: 0.01},\n",
       " ('Fri', 0): {0: 0.01, 100: 0.01, 200: 0.96, 300: 0.01, 400: 0.01},\n",
       " ('Fri', 100): {0: 0.01, 100: 0.96, 200: 0.01, 300: 0.01, 400: 0.01},\n",
       " ('Fri', 200): {0: 0.96, 100: 0.01, 200: 0.01, 300: 0.01, 400: 0.01},\n",
       " ('Fri', 300): {0: 0.96, 100: 0.01, 200: 0.01, 300: 0.01, 400: 0.01},\n",
       " ('Weekend', 0): {0: 0.2, 100: 0.2, 200: 0.2, 300: 0.2, 400: 0.2},\n",
       " ('Weekend', 100): {0: 0.2, 100: 0.2, 200: 0.2, 300: 0.2, 400: 0.2},\n",
       " ('Weekend', 200): {0: 0.2, 100: 0.2, 200: 0.2, 300: 0.2, 400: 0.2},\n",
       " ('Weekend', 300): {0: 0.2, 100: 0.2, 200: 0.2, 300: 0.2, 400: 0.2}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy, Q, Q_n = on_policy_first_visit_mc(env, 300000, 0.05, 1)\n",
    "policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we found the optimal policy with on policy MC control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
