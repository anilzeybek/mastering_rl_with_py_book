{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from food_truck_env import FoodTruck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_update(env, v, s, prob_a, gamma):\n",
    "    \"\"\"\n",
    "    Returns ‚àëùúã(ùëé|ùë†) ‚àëùëù(ùë†‚Ä≤, ùëü|ùë†, ùëé)[ùëü + ùõæùë£(ùë†)] for given state s, action probabilites and current state-value function\n",
    "    \"\"\"\n",
    "    expected_value = 0\n",
    "    for a in prob_a:\n",
    "        prob_next_s_r = env.get_transition_prob(s, a)\n",
    "        for next_s, r in prob_next_s_r:\n",
    "            expected_value += prob_a[a] * prob_next_s_r[next_s, r] * (r + gamma * v[next_s])\n",
    "\n",
    "    return expected_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(env, policy, max_iter=100, v=None, eps=0.1, gamma=1):\n",
    "    \"\"\"\n",
    "    Calls expected_update for each state in given policy multiple times, until convergence\n",
    "    And returns state-value function v\n",
    "    \"\"\"\n",
    "    if not v:\n",
    "        v = {s: 0 for s in env.state_space}\n",
    "\n",
    "    k = 0\n",
    "    while True:\n",
    "        max_delta = 0\n",
    "        for s in v:\n",
    "            if not env.is_terminal(s):\n",
    "                v_old = v[s] # The reason we keep v_old is just to measure delta, which is used to terminate the function\n",
    "                prob_a = policy[s]\n",
    "                v[s] = expected_update(env, v, s, prob_a, gamma)\n",
    "\n",
    "                max_delta = max(max_delta, abs(v[s] - v_old))\n",
    "\n",
    "        k += 1\n",
    "        if max_delta < eps:\n",
    "            print(f\"Converged in {k} iterations\")\n",
    "            break\n",
    "        elif k == max_iter:\n",
    "            print(f\"Terminating after {k} iterations\")\n",
    "            break\n",
    "\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def some_policy(states):\n",
    "    policy = {}\n",
    "    for s in states:\n",
    "        day, inventory = s\n",
    "        prob_a = {}\n",
    "        \n",
    "        if inventory >= 300:\n",
    "            prob_a[0] = 1\n",
    "        else:\n",
    "            prob_a[200 - inventory] = 0.5\n",
    "            prob_a[300 - inventory] = 0.5\n",
    "\n",
    "        policy[s] = prob_a\n",
    "    \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_truck = FoodTruck()\n",
    "policy = some_policy(food_truck.state_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 6 iterations\n",
      "Expected weekly profit for some policy is:  2515.0\n"
     ]
    }
   ],
   "source": [
    "v = policy_evaluation(food_truck, policy)\n",
    "print(\"Expected weekly profit for some policy is: \", v[\"Mon\", 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets simulate the environment to see if we can really get 2515"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, policy):\n",
    "    prob_a = policy[state]\n",
    "    action = np.random.choice(a=list(prob_a.keys()), p=list(prob_a.values()))\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_policy(policy, n_episodes):\n",
    "    env = FoodTruck()\n",
    "    rewards = []\n",
    "\n",
    "    for i_episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        ep_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            action = choose_action(state, policy)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "\n",
    "            ep_reward += reward\n",
    "\n",
    "        rewards.append(ep_reward)\n",
    "\n",
    "    print(f\"Expected weekly profit: {np.mean(rewards)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected weekly profit: 2629.0\n"
     ]
    }
   ],
   "source": [
    "simulate_policy(policy, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that simulation gives very similar result, now lets use policy iteration to find better policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(env, v, s, actions, gamma):\n",
    "    \"\"\"\n",
    "    Maps the given state to an action based on the current value-function\n",
    "    Mapping based on best action that can be taken from that state\n",
    "    \"\"\"\n",
    "    prob_a = {}\n",
    "    if not env.is_terminal(s):\n",
    "        max_q = np.NINF\n",
    "        best_a = None\n",
    "\n",
    "        for a in actions:\n",
    "            q_sa = expected_update(env, v, s, {a: 1}, gamma)\n",
    "            if q_sa >= max_q:\n",
    "                max_q = q_sa\n",
    "                best_a = a\n",
    "\n",
    "        prob_a[best_a] = 1\n",
    "    else:\n",
    "        # Terminal state has always state value 1 since we can not get any reward from that state\n",
    "        max_q = 0\n",
    "\n",
    "    return prob_a, max_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env, eps=0.1, gamma=1):\n",
    "    states = env.state_space\n",
    "    actions = env.action_space\n",
    "\n",
    "    policy = {s: {np.random.choice(actions): 1} for s in states}\n",
    "    v = {s: 0 for s in states}\n",
    "\n",
    "    while True:\n",
    "        v = policy_evaluation(env, policy, v=v, eps=eps, gamma=gamma)\n",
    "        old_policy = policy\n",
    "\n",
    "        policy = {}\n",
    "        for s in states:\n",
    "            policy[s], _ = policy_improvement(env, v, s, actions, gamma)\n",
    "\n",
    "        if old_policy == policy:\n",
    "            break\n",
    "    \n",
    "    print(\"Optimal policy found!\")\n",
    "    return policy, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 6 iterations\n",
      "Converged in 6 iterations\n",
      "Converged in 5 iterations\n",
      "Optimal policy found!\n",
      "Expected weekly profit for optimal policy: 2880.0\n"
     ]
    }
   ],
   "source": [
    "optimal_policy, v = policy_iteration(food_truck)\n",
    "print(f\"Expected weekly profit for optimal policy: {v['Mon', 0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We successfully found the optimal policy using policy iteration, but we achieve the same thing with value iteration that is more efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, max_iter=100, eps=0.1, gamma=1):\n",
    "    states = env.state_space\n",
    "    actions = env.action_space\n",
    "\n",
    "    v = {s: 0 for s in states}\n",
    "    policy = {}\n",
    "\n",
    "    k = 0\n",
    "    while True:\n",
    "        max_delta = 0\n",
    "        for s in states:\n",
    "            old_v = v[s]\n",
    "            policy[s], v[s] = policy_improvement(env, v, s, actions, gamma)\n",
    "            \n",
    "            max_delta = max(max_delta, abs(v[s] - old_v))\n",
    "\n",
    "        k += 1\n",
    "        if max_delta < eps:\n",
    "            print(f\"Converged in {k} iterations\")\n",
    "            break\n",
    "        elif k == max_iter:\n",
    "            print(f\"Terminating after {k} iterations\")\n",
    "            break\n",
    "\n",
    "    return policy, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 6 iterations\n",
      "Expected weekly profit for optimal policy: 2880.0\n"
     ]
    }
   ],
   "source": [
    "optimal_policy, v = value_iteration(food_truck)\n",
    "print(f\"Expected weekly profit for optimal policy: {v['Mon', 0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
